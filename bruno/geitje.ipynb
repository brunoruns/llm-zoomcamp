{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geitje model\n",
    "\n",
    "deze notebook heeft als doel het geitje model lokaal te draaien, via Docker, met een streamlit eraan vastgemaakt, zodat deze setup kan worden gedeployed naar een Hetzner high compute server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Mistral(\n",
    "    server_url='http://localhost:11434/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wil je een document meegeven ? TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_no_docs(query):\n",
    "    prompt_template = \"\"\"\n",
    "    Je bent een spellings- en grammatica verbeteraar. Verbeter altijd de spelling voor de ingevoerde tekst, en suggereer verbeteringen qua woordgebruik.\n",
    "    Geef nooit antwoorden op vragen, je gaat enkel over SPELLING en GRAMMATICA, van de gegeven TEKST:\n",
    "\n",
    "    TEKST: {question}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    \n",
    "    prompt = prompt_template.format(question=query).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geitje = \"tommy/geitje:7b_q8_0\"\n",
    "def llm(prompt):\n",
    "    response = client.chat.complete(\n",
    "        model=geitje,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    #search_results = elastic_search(query)\n",
    "    prompt = build_prompt_no_docs(query)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SDKError",
     "evalue": "API error occurred: Status 404\n{\"error\":{\"message\":\"model 'tommy/geitje' not found, try pulling it first\",\"type\":\"api_error\",\"param\":null,\"code\":null}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m rag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDit is een duidelijke spelfaut.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mrag\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrag\u001b[39m(query):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#search_results = elastic_search(query)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m build_prompt_no_docs(query)\n\u001b[0;32m----> 4\u001b[0m     answer \u001b[38;5;241m=\u001b[39m llm(prompt)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mllm\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm\u001b[39m(prompt):\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcomplete(\n\u001b[1;32m      4\u001b[0m         model\u001b[38;5;241m=\u001b[39mgeitje,\n\u001b[1;32m      5\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-zoomcamp/lib/python3.12/site-packages/mistralai/chat.py:120\u001b[0m, in \u001b[0;36mChat.complete\u001b[0;34m(self, model, messages, temperature, top_p, max_tokens, min_tokens, stream, stop, random_seed, response_format, tools, tool_choice, safe_prompt, retries, server_url, timeout_ms)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mHTTPValidationError(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4XX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res\u001b[38;5;241m.\u001b[39mtext, http_res)\n\u001b[1;32m    122\u001b[0m content_type \u001b[38;5;241m=\u001b[39m http_res\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected response received (code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhttp_res\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res\u001b[38;5;241m.\u001b[39mtext, http_res)\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 404\n{\"error\":{\"message\":\"model 'tommy/geitje' not found, try pulling it first\",\"type\":\"api_error\",\"param\":null,\"code\":null}}\n"
     ]
    }
   ],
   "source": [
    "answer = rag(\"Dit is een duidelijke spelfaut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
